{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BZ6-NSkBUiJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "# From http://www.ssp.sp.gov.br/transparenciassp/Consulta.aspx SP Dados Criminais\n",
        "data_csv = pd.read_csv('/content/drive/My Drive/crime stats sp/SPDadosCriminais_2023.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "Vu19oR30s9bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the city data\n",
        "# From http://blog.mds.gov.br/redesuas/lista-de-municipios-brasileiros/\n",
        "city_data = pd.read_csv('/content/drive/My Drive/crime stats sp/Lista_Municípios_com_IBGE_Brasil_Versao_CSV.csv', delimiter=';', encoding='ISO-8859-1')\n",
        "\n",
        "# Filter for cities in São Paulo state\n",
        "sp_cities = city_data[city_data['UF'] == 'SP']['Município'].unique()"
      ],
      "metadata": {
        "id": "6v0OBBHHpv6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O plano era usar o nome das delegacias, consultar no bing e então adicionar o lat/lon. No fim, ficou dado demais pra mostrar e os pings ficariam grudados. Uma parte do código ficou inútil, incluindo o mapping de acrônimo, mas vou deixar porque se entendi bem uma parte das delegacias de S.Paulo são de outras cidades (eu não plotei, sempre dá OOM com muita coisa)."
      ],
      "metadata": {
        "id": "c3SpHzlRm627"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping\n",
        "acronym_mapping = {\n",
        "    'DEL.': 'Delegacia ',\n",
        "    'POL.': 'Policia ',\n",
        "    'SEC.': 'Seccional ',\n",
        "    'SEC': 'Seccional ',\n",
        "    'DEL': 'Delegacia ',\n",
        "    'POL': 'Policia ',\n",
        "    'DDM': 'Delegacia Defesa da Mulher ',\n",
        "    'D.P.': 'Delegacia de Policia ',\n",
        "    'D.P': 'Delegacia de Policia ',\n",
        "    'CPJ': 'Central de Policia Judiciaria ',\n",
        "    'DHPP': 'Departamento de Homicídios e Proteção à Pessoa ',\n",
        "    'DISE': 'Delegacia de Investigações Sobre Entorpecentes ',\n",
        "    'DOPE': 'Departamento de Operações Policiais Estratégicas ',\n",
        "    'DEIC': 'Departamento Estadual de Investigações Criminais ',\n",
        "    'DIG': 'Delegacia de Investigações Gerais ',\n",
        "    'SIG': 'Setor de Investigações Gerais ',\n",
        "    'S.': 'SAO ',\n",
        "    'PRES.': 'PRESIDENTE ',\n",
        "    'SAO ANTONIO DO PINHAL': 'SANTO ANTONIO DO PINHAL ',\n",
        "    'STA': 'SANTA ',\n",
        "}\n",
        "\n",
        "\n",
        "def replace_acronyms(string):\n",
        "    for acronym, full_form in acronym_mapping.items():\n",
        "        # Replace the acronym wherever it appears in the string\n",
        "        string = re.sub(f\"\\\\b{acronym}\\\\b\", full_form, string)\n",
        "    return string\n",
        "\n",
        "data_csv['NOME_DELEGACIA_EXPANDED'] = data_csv['NOME_DELEGACIA'].dropna().apply(replace_acronyms)\n",
        "# Replace any number of consecutive spaces with a single space in the 'NOME_DELEGACIA' column\n",
        "data_csv['NOME_DELEGACIA_EXPANDED'] = data_csv['NOME_DELEGACIA_EXPANDED'].str.replace(r'\\s+', ' ')\n"
      ],
      "metadata": {
        "id": "1G9ql9MGGnrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como São Paulo é S.Paulo, ele não vai remover."
      ],
      "metadata": {
        "id": "M0bRB_pEnpDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode\n",
        "\n",
        "# Remove accents from city names\n",
        "sp_cities_unaccented = [unidecode(city) for city in sp_cities]\n",
        "\n",
        "# Concatenate all city names into a regex pattern, with word boundaries\n",
        "city_pattern = r'\\b(' + '|'.join(sp_cities_unaccented) + r')\\b'\n",
        "\n",
        "# Create a new column in the dataframe that is True if the unaccented police station name matches the city pattern\n",
        "data_csv['Other_City'] = data_csv['NOME_DELEGACIA_EXPANDED'].apply(lambda x: unidecode(str(x))).str.contains(city_pattern, case=False, na=False)\n",
        "\n",
        "# Filter the dataframe to only include rows where 'Other_City' is False\n",
        "data_csv_filtered = data_csv[~data_csv['Other_City']]\n",
        "\n",
        "# Drop the 'Other_City' column as it is no longer needed\n",
        "data_csv_filtered = data_csv_filtered.drop(columns=['Other_City'])\n"
      ],
      "metadata": {
        "id": "vzWDOmXaqVpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the data for São Paulo\n",
        "data_sp = data_csv_filtered[data_csv_filtered['CIDADE'] == 'S.PAULO']\n",
        "\n",
        "# Group by month, delegacia, and natureza_apurada (type of crime)\n",
        "crime_stats = data_sp.groupby(['MES_ESTATISTICA', 'NOME_DELEGACIA_EXPANDED', 'NATUREZA_APURADA']).size().reset_index(name='COUNT')\n",
        "\n",
        "# Display the result\n",
        "print(crime_stats)"
      ],
      "metadata": {
        "id": "IX7W2PbXGAqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Só útil pra consulta no bing"
      ],
      "metadata": {
        "id": "T-LPyif7n9Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_geolocation(api_key, address):\n",
        "    base_url = \"http://dev.virtualearth.net/REST/v1/Locations\"\n",
        "    params = {\n",
        "        \"query\": str(address) + \", São Paulo, Brazil\",\n",
        "        \"key\": api_key,\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data['resourceSets'] and data['resourceSets'][0]['resources']:\n",
        "            lat = data['resourceSets'][0]['resources'][0]['point']['coordinates'][0]\n",
        "            lng = data['resourceSets'][0]['resources'][0]['point']['coordinates'][1]\n",
        "            return lat, lng\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "pmoF9qEODDAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique police station names\n",
        "stations = data_sp['NOME_DELEGACIA_EXPANDED'].unique()"
      ],
      "metadata": {
        "id": "oEC6hnL5EAZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso queira voltar a consulta do bing, por agora deixei comentado"
      ],
      "metadata": {
        "id": "lggSZIPmoBak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the unique police station names\n",
        "# stations = data_sp['NOME_DELEGACIA_EXPANDED'].unique()\n",
        "\n",
        "# # Create a dictionary to store the results\n",
        "# station_locations = {}\n",
        "\n",
        "# # For each station\n",
        "# for station in stations:\n",
        "#     # Geocode the station\n",
        "#     lat, lon = get_geolocation(\"api-key\", station)\n",
        "#     # Store the result\n",
        "#     station_locations[station] = (lat, lon)\n"
      ],
      "metadata": {
        "id": "-RoBowTlDFaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # If 'LATITUDE' or 'LONGITUDE' are null, replace with values from station_locations\n",
        "# data_sp['LATITUDE_COMB'] = np.where(pd.isnull(data_sp['LATITUDE']),\n",
        "#                                data_sp['NOME_DELEGACIA_EXPANDED'].map(lambda x: station_locations.get(x, (None, None))[0]),\n",
        "#                                data_sp['LATITUDE'])\n",
        "\n",
        "# data_sp['LONGITUDE_COMB'] = np.where(pd.isnull(data_sp['LONGITUDE']),\n",
        "#                                 data_sp['NOME_DELEGACIA_EXPANDED'].map(lambda x: station_locations.get(x, (None, None))[1]),\n",
        "#                                 data_sp['LONGITUDE'])\n"
      ],
      "metadata": {
        "id": "8OWdEujd7AJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keplergl"
      ],
      "metadata": {
        "id": "CjYLj8ZqRYOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'Latitude' and 'Longitude' columns from 'LATITUDE' and 'LONGITUDE'\n",
        "data_sp['Latitude'] = data_sp['LATITUDE']\n",
        "data_sp['Longitude'] = data_sp['LONGITUDE']\n"
      ],
      "metadata": {
        "id": "KaR7ahbqTH-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sp_fil = data_sp[(data_sp['LATITUDE'] != 0) & (data_sp['LONGITUDE'] != 0)]"
      ],
      "metadata": {
        "id": "_5_BlMmpYL5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the map center and zoom level\n",
        "config = {\n",
        "    'version': 'v1',\n",
        "    'config': {\n",
        "        'mapState': {\n",
        "            'latitude': -23.5505,\n",
        "            'longitude': -46.6333,\n",
        "            'zoom': 11\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create a Kepler.gl map\n",
        "map_1 = KeplerGl(height=500, config=config)\n",
        "\n",
        "data_sample = data_sp_fil\n",
        "\n",
        "data_subset = data_sample[['Latitude', 'Longitude', 'NATUREZA_APURADA']]\n",
        "\n",
        "# Add the data to the map\n",
        "map_1.add_data(data=data_subset, name=\"crimes\")"
      ],
      "metadata": {
        "id": "AwCqvFDnU4SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''\n",
        "  google.colab.widgets.installCustomManager('https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/6a14374f468a145a/manager.min.js');\n",
        "'''))\n",
        "map_1"
      ],
      "metadata": {
        "id": "jh-OAnGiU706"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvar no drive, se quiser, no diretório crime stats sp"
      ],
      "metadata": {
        "id": "EtrZhYXooYBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "map_1.save_to_html(file_name=\"/content/drive/MyDrive/crime stats sp/crime_map_sp.html\")"
      ],
      "metadata": {
        "id": "LD3Lz8oofenP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}